{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_8Lnd6DOUEPb"
      ],
      "toc_visible": true,
      "gpuType": "V28",
      "authorship_tag": "ABX9TyMgIAua7L43fVGOS04VYW5J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AntoPrinzi99/NN-project-Antonino-Prinzivalli/blob/main/AE_stitching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#AE stitching:"
      ],
      "metadata": {
        "id": "_8Lnd6DOUEPb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing"
      ],
      "metadata": {
        "id": "-fVBKnJKSkDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "yZe-iFPPSfZJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Download data saet"
      ],
      "metadata": {
        "id": "mRGjWVlXSnEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "eqan_SBDSgt_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#latent_dims = 2\n",
        "num_epochs = 50\n",
        "batch_size = 128\n",
        "capacity = 64\n",
        "learning_rate = 1e-3\n",
        "use_gpu = True\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "img_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # this will change the intervall of the image from 0,1  to -1,+1 , for this i use a tanh as activation function\n",
        "])\n",
        "\n",
        "train_dataset = MNIST(root='./data/MNIST', download=True, train=True, transform=img_transform)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # i just want to see if there are any differents in the embedding for other reasons\n",
        "\n",
        "test_dataset = MNIST(root='./data/MNIST', download=True, train=False, transform=img_transform)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7JN_q2kSjWh",
        "outputId": "db18fd2a-6667-4cd5-a3c6-779bfd9b0b58"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 14401452.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 491430.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 4389484.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 2370943.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Selecting the Anchors"
      ],
      "metadata": {
        "id": "2Or4AZnWSxnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_class_anchors(dataset, num_classes=10):\n",
        "    class_anchors = []\n",
        "    class_count = {i: 0 for i in range(num_classes)}\n",
        "\n",
        "    for data, target in dataset:\n",
        "        if all(count > 0 for count in class_count.values()):\n",
        "            break\n",
        "        if class_count[target] == 0:\n",
        "            class_anchors.append((data, target))\n",
        "            class_count[target] += 1\n",
        "\n",
        "    class_anchors.sort(key=lambda x: x[1])\n",
        "    anchor_images = torch.stack([item[0] for item in class_anchors])\n",
        "    return anchor_images\n",
        "\n",
        "# Extract anchor images from the training dataset\n",
        "anchors_images = get_class_anchors(train_dataset, num_classes=10)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "anchors_images = anchors_images.to(device)\n",
        "\n",
        "# print to verify\n",
        "print(anchors_images.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAcSa-GPS01t",
        "outputId": "3f8fc48e-46b7-4387-fb19-88af8de2e105"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Defination of the relative model:"
      ],
      "metadata": {
        "id": "OUdIevP_SrIa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NqQWM29WRtWD"
      },
      "outputs": [],
      "source": [
        "capacity = 64\n",
        "latent_dims = 10\n",
        "batch_size = 64\n",
        "num_anchors = 10\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, anchors):\n",
        "        super(Encoder, self).__init__()\n",
        "        c = capacity\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=c, kernel_size=4, stride=2, padding=1) # out: c x 14 x 14\n",
        "        self.conv2 = nn.Conv2d(in_channels=c, out_channels=c*2, kernel_size=4, stride=2, padding=1) # out: c x 7 x 7\n",
        "        self.fc = nn.Linear(in_features=c*2*7*7, out_features=latent_dims)\n",
        "        self.anchors = anchors\n",
        "        self.fc_relative_to_latent = nn.Linear(num_anchors, latent_dims)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = x.view(x.size(0), -1) # flatten batch of multi-channel feature maps to a batch of feature vectors\n",
        "        x = self.fc(x)\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            anchors_embeddings = self.encode_anchors(self.anchors)\n",
        "\n",
        "        relative_latent = self.relative_projection(x, anchors_embeddings)  #HERE I DO THE TRANSFORMATION\n",
        "\n",
        "\n",
        "        return relative_latent\n",
        "\n",
        "    def encode_anchors(self, anchors_images):\n",
        "        with torch.no_grad():\n",
        "            x = F.relu(self.conv1(anchors_images))\n",
        "            x = F.relu(self.conv2(x))\n",
        "            x = x.view(x.size(0), -1) # flatten batch of multi-channel feature maps to a batch of feature vectors\n",
        "            x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def relative_projection(self, x, anchors):\n",
        "        x = F.normalize(x, p=2, dim=-1)\n",
        "        anchors = F.normalize(anchors, p=2, dim=-1)\n",
        "        return torch.einsum(\"bm, am -> ba\", x, anchors)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        c = capacity\n",
        "        self.fc = nn.Linear(in_features=latent_dims, out_features=c*2*7*7)\n",
        "        self.conv2 = nn.ConvTranspose2d(in_channels=c*2, out_channels=c, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv1 = nn.ConvTranspose2d(in_channels=c, out_channels=1, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = x.view(x.size(0), capacity*2, 7, 7) # unflatten batch of feature vectors to a batch of multi-channel feature maps\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = torch.tanh(self.conv1(x)) # last layer before output is tanh, since the images are normalized and 0-centered\n",
        "        return x\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, anchors):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = Encoder(anchors)\n",
        "        self.decoder = Decoder()\n",
        "\n",
        "    def forward(self, x):\n",
        "        latent = self.encoder(x)\n",
        "        x_recon = self.decoder(latent)\n",
        "        return x_recon\n",
        "\n",
        "\n",
        "# Initialise autoencoder with the anchors\n",
        "autoencoder = Autoencoder(anchors_images).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training"
      ],
      "metadata": {
        "id": "baQsh99RUAve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###First encoder:"
      ],
      "metadata": {
        "id": "GplTCV7QUauQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialising the trainer\n",
        "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-3)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    autoencoder.train()\n",
        "    running_loss = 0.0\n",
        "    for data in train_dataloader:\n",
        "        inputs, _ = data\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = autoencoder(inputs)\n",
        "        loss = criterion(outputs, inputs)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_dataloader)}')\n",
        "\n",
        "\n",
        "num_params = sum(p.numel() for p in autoencoder.parameters() if p.requires_grad)\n",
        "print('Number of parameters: %d' % num_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkINnvdGUCId",
        "outputId": "4f328805-8058-42b1-9c4c-365fafd1722e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.2424977400155464\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Second one:"
      ],
      "metadata": {
        "id": "oLbxqDoKUgkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# i define the second AE\n",
        "autoencoder2 = Autoencoder(anchors_images).to(device)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(autoencoder2.parameters(), lr=1e-3)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    autoencoder2.train()\n",
        "    running_loss = 0.0\n",
        "    for data in train_dataloader:\n",
        "        inputs, _ = data\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = autoencoder2(inputs)\n",
        "        loss = criterion(outputs, inputs)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_dataloader)}')\n",
        "\n",
        "\n",
        "num_params = sum(p.numel() for p in autoencoder2.parameters() if p.requires_grad)\n",
        "print('Number of parameters: %d' % num_params)"
      ],
      "metadata": {
        "id": "rYK2RUkqUir7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##test of both encoders:"
      ],
      "metadata": {
        "id": "GPYbk0lkUTkS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###First Encoder"
      ],
      "metadata": {
        "id": "FkrT2LxYUsBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.ion()\n",
        "\n",
        "import torchvision.utils\n",
        "\n",
        "autoencoder.eval()\n",
        "\n",
        "\n",
        "def to_img(x):\n",
        "    x = 0.5 * (x + 1)\n",
        "    x = x.clamp(0, 1)\n",
        "    return x\n",
        "\n",
        "def show_image(img):\n",
        "    img = to_img(img)\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "def visualise_output(images, model):\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        images = images.to(device)\n",
        "        images = model(images)\n",
        "        images = images.cpu()\n",
        "        images = to_img(images)\n",
        "        np_imagegrid = torchvision.utils.make_grid(images[1:50], 10, 5).numpy()\n",
        "        plt.imshow(np.transpose(np_imagegrid, (1, 2, 0)))\n",
        "        plt.show()\n",
        "\n",
        "images, labels = next(iter(test_dataloader))\n",
        "\n",
        "# First visualise the original images\n",
        "print('Original images')\n",
        "show_image(torchvision.utils.make_grid(images[1:50],10,5))\n",
        "plt.show()\n",
        "\n",
        "# Reconstruct and visualise the images using the autoencoder\n",
        "print('Autoencoder reconstruction:')\n",
        "visualise_output(images, autoencoder)"
      ],
      "metadata": {
        "id": "IafF-p8bUVjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Second Encoder"
      ],
      "metadata": {
        "id": "6IcCRYrHUuLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.ion()\n",
        "\n",
        "import torchvision.utils\n",
        "\n",
        "autoencoder2.eval()\n",
        "\n",
        "# This function takes as an input the images to reconstruct\n",
        "# and the name of the model with which the reconstructions\n",
        "# are performed\n",
        "def to_img(x):\n",
        "    x = 0.5 * (x + 1)\n",
        "    x = x.clamp(0, 1)\n",
        "    return x\n",
        "\n",
        "def show_image(img):\n",
        "    img = to_img(img)\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "def visualise_output(images, model):\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        images = images.to(device)\n",
        "        images = model(images)\n",
        "        images = images.cpu()\n",
        "        images = to_img(images)\n",
        "        np_imagegrid = torchvision.utils.make_grid(images[1:50], 10, 5).numpy()\n",
        "        plt.imshow(np.transpose(np_imagegrid, (1, 2, 0)))\n",
        "        plt.show()\n",
        "\n",
        "images, labels = next(iter(test_dataloader))\n",
        "\n",
        "# First visualise the original images\n",
        "print('Original images')\n",
        "show_image(torchvision.utils.make_grid(images[1:50],10,5))\n",
        "plt.show()\n",
        "\n",
        "# Reconstruct and visualise the images using the autoencoder\n",
        "print('Autoencoder reconstruction:')\n",
        "visualise_output(images, autoencoder2)"
      ],
      "metadata": {
        "id": "DuvY2ECXUpvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Now i do the stitch:"
      ],
      "metadata": {
        "id": "JAzvtExHU58g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "just save the models:"
      ],
      "metadata": {
        "id": "EmDaRQ6IZCgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save(autoencoder.state_dict(), 'autoencoder_weights.pth')\n",
        "#torch.save(autoencoder2.state_dict(), 'autoencoder2_weights.pth')"
      ],
      "metadata": {
        "id": "Gd-4jFSaZD6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#autoencoder3=Autoencoder(anchors_images).to(device)\n",
        "#autoencoder4=Autoencoder(anchors_images).to(device)\n",
        "#autoencoder3.load_state_dict(torch.load('/content/autoencoder_weights.pth'))\n",
        "#autoencoder4.load_state_dict(torch.load('/content/autoencoder2_weights.pth'))\n"
      ],
      "metadata": {
        "id": "oiRLAX32ZTVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StitchedAutoencoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(StitchedAutoencoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, x):\n",
        "        latent = self.encoder(x)\n",
        "        print(f\"latent shape: {latent.shape}\")\n",
        "        decoded = self.decoder(latent)\n",
        "        print(f\"decoded shape: {decoded.shape}\")\n",
        "        return decoded"
      ],
      "metadata": {
        "id": "PNNREju3U7r8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure models are in evaluation mode\n",
        "autoencoder.eval()\n",
        "autoencoder2.eval()\n",
        "stitched_autoencoder = StitchedAutoencoder(autoencoder.encoder, autoencoder2.decoder).to(device)"
      ],
      "metadata": {
        "id": "eQlF1E3SVbTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stitched_autoencoder.eval()\n",
        "\n",
        "def to_img(x):\n",
        "    x = 0.5 * (x + 1)\n",
        "    x = x.clamp(0, 1)\n",
        "    return x\n",
        "\n",
        "def show_image(img):\n",
        "    img = to_img(img)\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "def visualise_output(images, model):\n",
        "    with torch.no_grad():\n",
        "        images = images.to(device)\n",
        "        images = model(images)\n",
        "        images = images.cpu()\n",
        "        images = to_img(images)\n",
        "        np_imagegrid = torchvision.utils.make_grid(images[1:50], 10, 5).numpy()\n",
        "        plt.imshow(np.transpose(np_imagegrid, (1, 2, 0)))\n",
        "        plt.show()\n",
        "\n",
        "images, labels = next(iter(test_dataloader))\n",
        "\n",
        "# First visualise the original images\n",
        "print('Original images')\n",
        "show_image(torchvision.utils.make_grid(images[1:50], 10, 5))\n",
        "plt.show()\n",
        "\n",
        "# Reconstruct and visualise the images using the stitched autoencoder\n",
        "print('Autoencoder reconstruction:')\n",
        "visualise_output(images, stitched_autoencoder)"
      ],
      "metadata": {
        "id": "x61k9MTJVgWJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}